project_path:  /home/hermannl/phd/repos/affordance_rl
data_path: ${project_path}/VREnv/data/
save_dir: ./combined/hydra_outputs/${task}/${now:%Y-%m-%d}/
model_name: real_world
task: pickup
euler_obs: true
img_size: 64

viz_obs: False
# path to load affordance models either gripper_cam 
# or static_cam
models_path: "${project_path}/Master-project/trained_models/"
resume_training: True
resume_model_path: ${save_dir} # ./combined/hydra_outputs/${task}/2021-09-15_ours

# Static cam affordance model to detect the targets
n_objects: 4
target_search_aff:
  model_path: ${models_path}/static_real_world.ckpt
  img_size: 200
  hyperparameters:
    n_classes: 2
    hough_voting:
      skip_pixels: 2
      inlier_threshold: 0.7
      angle_discretization: 100
      inlier_distance: 10
      percentage_threshold: 0.3
      object_center_kernel_radius: 15
    centers_loss:
      weight: 5
    aff_loss:
      weight: 2
      dice_loss:
        add: True
        weight: 5
      ce_loss:
        weight: 1
        class_weights: [0.2, 0.8]
    optimizer:
      lr: 1e-5
      weight_decay: 1e-3
    unet_cfg:
      decoder_channels: [128, 64, 32]

target_search: "real_world"
# Define types of observation input to the RL agent
env_wrapper:
  gripper_cam:
    use_img: True
    use_depth: True
  static_cam:
    use_img: False
    use_depth: False
  use_pos: True
  img_size: ${img_size}
  save_images: False

panda_env_wrapper:
  d_pos: 0.005
  d_rot: 0.3
  gripper_success_threshold: 0.01
  reward_fail: -20
  reward_success: 200
  termination_radius: 0.10

# Affordance configuration for RL agent observation inputs
affordance:
    static_cam:
      use: False
      model_path: ${models_path}/static_real_world.ckpt
      img_size: 200
      hough_voting:
        skip_pixels: 2
        inlier_threshold: 0.7
        angle_discretization: 100
        inlier_distance: 10
        percentage_threshold: 0.3
        object_center_kernel_radius: 10
      hyperparameters:
        n_classes: 2
        centers_loss:
          weight: 2.5
        aff_loss:
          weight: 1
          dice_loss:
            add: True
            weight: 5
          ce_loss:
            weight: 1
            class_weights: [0.2, 0.8]
        optimizer:
          lr: 1e-5
          weight_decay: 1e-3
        unet_cfg:
          decoder_channels: [128, 64, 32]
    gripper_cam:
      use: True  # Use affordance in observation
      use_distance: True
      densify_reward: True  # Use affordance to shape the reward function
      target_in_obs: False  # Add target detected by affordance into observation
      model_path: ${models_path}/gripper_real_world.ckpt
      hough_voting:
        skip_pixels: 2
        inlier_threshold: 0.7
        angle_discretization: 100
        inlier_distance: 10
        percentage_threshold: 0.3
        object_center_kernel_radius: 10
      hyperparameters:
        n_classes: 2
        centers_loss:
          weight: 2.5
        aff_loss:
          weight: 1
          dice_loss:
            add: True
            weight: 5
          ce_loss:
            weight: 1
            class_weights: [0.2, 0.8]
        optimizer:
          lr: 1e-5
          weight_decay: 1e-3
        unet_cfg:
          decoder_channels: [128, 64, 32]

hydra:
  run:
    dir: ${save_dir}/

defaults:
  - robot_io/robot@robot: panda_frankx_interface
  - robot_io/env@robot_env: env
  - robot_io/cams@cams: camera_manager
  - robot_io/cams/static_cam@static_cam: kinect4_oier
  - agent: default
  - test: test_tabletop
  - transforms@affordance.transforms: aff_transforms
  - transforms@env_wrapper.transforms: rl_transforms
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog