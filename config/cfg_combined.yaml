project_path:  /mnt/16867D9A9C78B590/Users/Jessica/Documents/Proyecto_ssd/ # C:/Users/Jessica/Documents/Proyecto_ssd #
data_path: ${project_path}/VREnv/data/
save_dir: ./combined/hydra_outputs/${task}
model_name: "full"
task: pickup
euler_obs: true
img_size: 64
repeat_training: 1

# Define types of input to the RL agent
env_wrapper:
  use_depth: False
  use_pos: True
  use_static_cam: False
  use_gripper_cam: True
  history_length: 1
  skip_frames: 0
  img_size: ${img_size}
  transforms: ${transforms}

# path to load affordance models either gripper_cam 
# or static_cam
models_path: "${project_path}/Master-project/trained_models/"

# Static cam affordance model to detect the targets
target_search_aff:
  model_path: ${models_path}/static_cam_center_preds_1.ckpt
  img_size: 200
  hyperparameters:
    n_classes: 2
    hough_voting:
      skip_pixels: 2
      inlier_threshold: 0.7
      angle_discretization: 100
      inlier_distance: 10
      percentage_threshold: 0.3
      object_center_kernel_radius: 10
    centers_loss:
      weight: 5
    aff_loss:
      weight: 2
      dice_loss:
        add: True
        weight: 5
      ce_loss:
        weight: 1
        class_weights: [0.2, 0.8]
    optimizer:
      lr: 1e-5
      weight_decay: 1e-3
    unet_cfg:
      decoder_channels: [128, 64, 32]

# RL agent
affordance:
    hyperparameters:
      n_classes: 2
      centers_loss:
        weight: 2.5
      aff_loss:
        weight: 1
        dice_loss:
          add: True
          weight: 5
        ce_loss:
          weight: 1
          class_weights: [0.2, 0.8]
      optimizer:
        lr: 1e-5
        weight_decay: 1e-3
      unet_cfg:
        decoder_channels: [128, 64, 32]
    static_cam:
      use: False
      model_path: ${models_path}/static_cam_1ce_5dice.ckpt
    gripper_cam:
      use: False  # Use affordance in observation
      densify_reward: False  # Use affordance to shape the reward function
      target_in_obs: False  # Add target detected by affordance into observation
      model_path: ${models_path}/gripper_centers_2-5_1_5_1_64px_epoch_40.ckpt
      hough_voting:
        skip_pixels: 2
        inlier_threshold: 0.7
        angle_discretization: 100
        inlier_distance: 10
        percentage_threshold: 0.3
        object_center_kernel_radius: 10

defaults:
  - robot: panda_tabletop
  - scene: clutter_tabletop
  - env: env_combined
  - env@eval_env: env_combined
  - camera_conf: tabletop_topview
  - agent: default
  - test: test_combined
  - transforms: img_transforms
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog

all_cameras:
  sideview_right: #Side view (slide)
    _target_: vr_env.camera.static_camera.StaticCamera
    name: static_right
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    look_at: [ 0.2, 0.8, 0.6 ]
    look_from: [0.8, 0.2, 1]
  sideview_left: #Side view (drawer)
    _target_: vr_env.camera.static_camera.StaticCamera
    name: static_left
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    look_at: [ 0.05, 0.8, 0.6 ]
    look_from: [-0.5, 0.2, 1]
  topview:    
    _target_: vr_env.camera.static_camera.StaticCamera
    name: static_topview
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    look_at: [ 0.2, 0.85, 0.5 ]
    look_from: [0.2, 0.05, 1.3]
  tabletop_topview:
    _target_: vr_env.camera.static_camera.StaticCamera
    name: tabletop_topview
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    look_at: [0.3, 0.8, 0.5]
    look_from: [0.3, 0.6, 1.5]
  static:
    _target_: vr_env.camera.static_camera.StaticCamera
    name: static
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    look_at: [ 0.1, 0.8, 0.7 ]
    look_from: [ 0.3, -0.2, 1.3 ]
  gripper:
    _target_: vr_env.camera.gripper_camera.GripperCamera
    name: gripper
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    gripper_cam_link: 12 # TODO do not hardcode this, use robot property? e.g. ${robot.camera_link_id}
  opposing:
    _target_: vr_env.camera.static_camera.StaticCamera
    name: opposing
    fov: 75
    aspect: 1
    nearval: 0.01
    farval: 2
    width: 300
    height: 300
    look_at: [ 0.4, 0.5, 0.6 ]
    look_from: [ 0.4, 1.5, 0.9 ]

hydra:
  run:
    dir: ${save_dir}./${now:%Y-%m-%d}/${now:%H-%M-%S}