data_path: ${paths.vr_data}
save_dir: ./hydra_outputs/${task}
model_name: "full"
task: slide
img_size: 64
repeat_training: 1

viz_obs: False
init_angle: False

# Static cam affordance model to detect the targets
target_search_aff:
  model_path: ${paths.trained_models}/static_playtable.ckpt
  img_size: 200
  hyperparameters:
    n_classes: 4
    hough_voting:
      skip_pixels: 2
      inlier_threshold: 0.7
      angle_discretization: 100
      inlier_distance: 10
      percentage_threshold: 0.3
      object_center_kernel_radius: 10
    centers_loss:
      weight: 5
    aff_loss:
      weight: 2
      dice_loss:
        add: True
        weight: 5
      ce_loss:
        weight: 1
        class_weights: [0.2, 0.8]
    optimizer:
      lr: 1e-5
      weight_decay: 1e-3
    unet_cfg:
      decoder_channels: [128, 64, 32]
#
target_search: "affordance"
# Define types of observation input to the RL agent
env_wrapper:
  gripper_cam:
    use_img: True
    use_depth: True
  static_cam:
    use_img: True
    use_depth: False
  use_pos: True
  img_size: ${img_size}
  use_aff_termination: False
  max_target_dist: 0.15
  use_env_state: False

# Affordance configuration for RL agent observation inputs
affordance:
    static_cam:
      use: False
      model_path: ${paths.trained_models}/static_playtable.ckpt
      img_size: 200
      hough_voting:
        skip_pixels: 2
        inlier_threshold: 0.7
        angle_discretization: 100
        inlier_distance: 10
        percentage_threshold: 0.3
        object_center_kernel_radius: 10
      hyperparameters:
        n_classes: 4
        centers_loss:
          weight: 2.5
        aff_loss:
          weight: 1
          dice_loss:
            add: True
            weight: 5
          ce_loss:
            weight: 1
            class_weights: [0.2, 0.8]
        optimizer:
          lr: 1e-5
          weight_decay: 1e-3
        unet_cfg:
          decoder_channels: [128, 64, 32]
    gripper_cam:
      use: False  # Use affordance in observation
      use_distance: False
      densify_reward: False  # Use affordance to shape the reward function
      target_in_obs: False  # Add target detected by affordance into observation
      model_path: ${paths.trained_models}/hinge_left_gripper.ckpt  # gripper_playtable_64px.ckpt
      hough_voting:
        skip_pixels: 2
        inlier_threshold: 0.7
        angle_discretization: 100
        inlier_distance: 10
        percentage_threshold: 0.3
        object_center_kernel_radius: 10
      hyperparameters:
        n_classes: 2
        centers_loss:
          weight: 2.5
        aff_loss:
          weight: 1
          dice_loss:
            add: True
            weight: 5
          ce_loss:
            weight: 1
            class_weights: [0.2, 0.8]
        optimizer:
          lr: 1e-5
          weight_decay: 1e-3
        unet_cfg:
          decoder_channels: [128, 64, 32]

hydra:
  run:
    dir: ${save_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}

defaults:
  - robot: panda
  - scene: empty_playtable_side
  - env: env_combined
  - env@eval_env: env_combined
  - camera_conf: playtable
  - agent: default_playtable
  - test: test_playtable
  - paths: general_paths
  - transforms@affordance.transforms: aff_transforms
  - transforms@env_wrapper.transforms: rl_transforms
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog